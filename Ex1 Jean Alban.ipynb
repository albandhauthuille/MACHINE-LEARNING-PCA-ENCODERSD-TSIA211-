{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iafPdtuncbq7"
   },
   "source": [
    "<h1><center>MNIST classification using Numpy<center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4VrCB5La5rD"
   },
   "source": [
    "## Importing Numpy and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlKZ3Hnas7B4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"Using tensorflow version \" + str(tf.__version__))\n",
    "print(\"Using keras version \" + str(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_QLz9_jbRZq"
   },
   "source": [
    "## Loading and preparing the MNIST dataset\n",
    "Load the MNIST dataset made available by keras.datasets. Check the size of the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "gG83hGyVmijn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "    taille de x_train: (60000, 28, 28)\n",
      "    taille de y_train: (60000,)\n",
      "\n",
      "Testing set:\n",
      "    taille de x_test: (10000, 28, 28)\n",
      "    taille de y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
    "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train),(x_test, y_test) =mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "\n",
    "\n",
    "# START CODE HERE\n",
    "print(\"Training set:\")\n",
    "print(\"    taille de x_train:\",np.shape(x_train))\n",
    "print(\"    taille de y_train:\",np.shape(y_train))\n",
    "print('\\nTesting set:')\n",
    "print(\"    taille de x_test:\",np.shape(x_test))\n",
    "print(\"    taille de y_test:\",np.shape(y_test))\n",
    "# END CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRPbU_Z4U6Ac"
   },
   "source": [
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "Using the pyplot package, visualize the first sample of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VAu7oW0Zu4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize the first training sample using the Matplotlib library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# START CODE HERE\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15,8))\n",
    "for i in range(9):\n",
    "    ax = axes[i//3, i%3]\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(\"image n°\"+str(i+1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7YsRekMVDg-"
   },
   "source": [
    "The database contains images of handwritten digits. Hence, they belong to one of 10 categories, depending on the digit they represent. \n",
    "Reminder: in order to do multi-class classification, we use the softmax function, which outputs a multinomial probability distribution. That means that the output to our model will be a vector of size $10$, containing probabilities (meaning that the elements of the vector will be positive sum to $1$).\n",
    "For easy computation, we want to true labels to be represented with the same format: that is what we call **one-hot encoding**. For example, if an image $\\mathbf{x}$ represents the digit $5$, we have the corresponding one_hot label (careful, $0$ will be the first digit): \n",
    "$$ \\mathbf{y} = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] $$\n",
    "Here, you need to turn train and test labels to one-hot encoding using the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "# START CODE HERE\n",
    "boolean = True\n",
    "if boolean:\n",
    "    y_train, y_test = to_categorical(y_train),to_categorical(y_test)\n",
    "    boolean = False\n",
    "y_train\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jv29YLtVO3q"
   },
   "source": [
    "Images are black and white, with size $28 \\times 28$. We will work with them using a simple linear classification model, meaning that we will have them as vectors of size $(784)$.\n",
    "You should then transform the images to the size $(784)$ using the numpy function ```reshape```.\n",
    "\n",
    "Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation. Be careful to your methodology: while you have access to training data, you may not have access to testing data, and must avoid using any statistic on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptTRSDo5nJyZ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c1a4d74084d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reshape to proper images with 1 color channel according to backend scheme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# START CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Reshape to proper images with 1 color channel according to backend scheme\n",
    "img_rows, img_cols = train_images_images.shape[1], x_train.shape[2]\n",
    "\n",
    "# START CODE HERE\n",
    "train_images = x_train.reshape(60000,-1)\n",
    "print(\"The new shape of the image is: \",np.shape(train_images))\n",
    "# END CODE HERE\n",
    "\n",
    "# Cast pixels from uint8 to float32\n",
    "train_images = train_images.astype('float32')\n",
    "\n",
    "# Now let us normalize the images so that they have zero mean and standard deviation\n",
    "# Hint: are real testing data statistics known at training time ?\n",
    "# START CODE HERE\n",
    "train_images = train_images.astype('float32')\n",
    "print(\"\\n\\nPrevious mean of the first image: \",np.mean(train_images[0]))\n",
    "print(\"Previous std of the first image: \",np.std(train_images[0]))\n",
    "\n",
    "for i,im in enumerate(train_images):\n",
    "    train_images[i] = (im-np.mean(im))/np.std(im)\n",
    "    \n",
    "print(\"\\nNew mean of the first image: \",np.mean(train_images[0]))\n",
    "print(\"New std of the first image: \",np.std(train_images[0]))\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Numpy\n",
    "\n",
    "Look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf) for some basic information on how to use numpy.\n",
    "\n",
    "## Defining the model \n",
    "\n",
    "We will here create a simple, linear classification model. We will take each pixel in the image as an input feature (making the size of the input to be $784$) and transform these features with a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$. Since there is $10$ possible classes, we want to obtain $10$ scores. Then, \n",
    "$$ \\mathbf{W} \\in \\mathbb{R}^{784 \\times 10} $$\n",
    "$$ \\mathbf{b} \\in \\mathbb{R}^{10} $$\n",
    "\n",
    "and our scores are obtained with:\n",
    "$$ \\mathbf{z} = \\mathbf{W}^{T} \\mathbf{x} +  \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^{784}$ is the input vector representing an image.\n",
    "We note $\\mathbf{y} \\in \\mathbb{R}^{10}$ as the target one_hot vector. \n",
    "\n",
    "Here, you fist need to initialize $\\mathbf{W}$ and $\\mathbf{b}$ using ```np.random.normal``` and ```np.zeros```, then compute $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid implementing a complicated gradient back-propagation,\n",
    "# we will try a very simple architecture with one layer \n",
    "def initLayer(n_input,n_output):\n",
    "    \"\"\"\n",
    "    Initialize the weights, return the number of parameters\n",
    "    Inputs: n_input: the number of input units - int\n",
    "          : n_output: the number of output units - int\n",
    "    Outputs: W: a matrix of weights for the layer - numpy ndarray\n",
    "           : b: a vector bias for the layer - numpy ndarray\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "    # START CODE HERE\n",
    "    W = np.random.normal(size = (n_input,n_output))\n",
    "    print(np.shape(W))\n",
    "    b = np.zeros(n_output)\n",
    "    nb_params = np.shape(W)[0]*np.shape(W)[1]+np.shape(b)[0]\n",
    "    # END CODE HERE\n",
    "    return W, b, nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n"
     ]
    }
   ],
   "source": [
    "n_training = train_images.shape[0] \n",
    "n_feature = len(train_images[0])\n",
    "n_labels = 10\n",
    "W, b, nb_params = initLayer(n_feature, n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, b, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: W: the weights - numpy ndarray\n",
    "          : b: the bias - numpy ndarray\n",
    "          : X: the batch - numpy ndarray\n",
    "    Outputs: z: outputs - numpy ndarray\n",
    "    \"\"\"\n",
    "    z = W.T@X + b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the output \n",
    "\n",
    "To obtain classification probabilities, we use the softmax function:\n",
    "$$ \\mathbf{o} = softmax(\\mathbf{z}) \\text{         with          } o_i = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)} $$\n",
    "\n",
    "The usual difficulty with the softmax function is the possibility of overflow when the scores $z_i$ are already large. Since a softmax is not affected by a shift affecting the whole vector $\\mathbf{z}$:\n",
    "$$ \\frac{\\exp(z_i - c)}{\\sum_{j=0}^{9} \\exp(z_j - c)} =  \\frac{\\exp(c) \\exp(z_i)}{\\exp(c) \\sum_{j=0}^{9} \\exp(z_j)} = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)}$$\n",
    "what trick can we use to ensure we will not encounter any overflow ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Perform the softmax transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - numpy ndarray\n",
    "    Outputs: out: the activation values - numpy ndarray\n",
    "    \"\"\"\n",
    "    # We can use the following trick: using z-max(zi) instead of simpply z. The output will be unchanged.\n",
    "    out = exp(z-max(z))/np.sum(exp(z-max(z)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making updates\n",
    "\n",
    "We define a learning rate $\\eta$. The goal is to be able to apply updates:\n",
    "$$ \\mathbf{W}^{t+1} = \\mathbf{W}^{t} + \\nabla_{\\mathbf{W}} l_{MLE} $$\n",
    "\n",
    "In order to do this, we will compute this gradient (and the bias) in the function ```update```. In the next function ```updateParams```, we will actually apply the update with regularization. \n",
    "\n",
    "Reminder: the gradient $\\nabla_{\\mathbf{W}} l_{MLE}$ is the matrix containing the partial derivatives \n",
    "$$ \\left[\\frac{\\delta l_{MLE}}{\\delta W_{ij}}\\right]_{i=1..784, j=1..10} $$\n",
    "**Remark**: Careful, the usual way of implementing this in python has the dimensions of $\\mathbf{W}$ reversed compared to the notation of the slides.\n",
    "\n",
    "Coordinate by coordinate, we obtain the following update: \n",
    "$$ W_{ij}^{t+1} = W_{ij}^{t} + \\eta \\frac{\\delta l_{MLE}}{\\delta W_{ij}} $$\n",
    "\n",
    "Via the chain rule, we obtain, for an input feature $i \\in [0, 783]$ and a output class $j \\in [0, 9]$: $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = \\frac{\\delta l_{MLE}}{\\delta z_{j}} \\frac{\\delta z_j}{\\delta W_{ij}}$$ \n",
    "\n",
    "It's easy to compute that $\\frac{\\delta z_j}{\\delta W_{ij}} = x_i$\n",
    "\n",
    "We compute the softmax derivative, to obtain:\n",
    "$$ \\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y} $$\n",
    "\n",
    "Hence, $\\frac{\\delta l_{MLE}}{\\delta z_{j}} = o_j - y_j$ and we obtain that $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = (o_j - y_j) x_i$$\n",
    "\n",
    "This can easily be written as a scalar product, and a similar computation (even easier, actually) can be done for $\\mathbf{b}$. Noting $\\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y}$ as ```grad``` in the following function, compute the gradients $\\nabla_{\\mathbf{W}} l_{MLE}$ and $\\nabla_{\\mathbf{b}} l_{MLE}$ in order to call the function ```updateParams```.\n",
    "\n",
    "Note: the regularizer and the weight_decay $\\lambda$ are used in ```updateParams```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, W, b, grad, X, regularizer, weight_decay):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : W: the weights - ndarray\n",
    "          : b: the bias -  ndarray\n",
    "          : grad: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : X: the data -  ndarray\n",
    "          : regularizer: 'L2' or None - the regularizer to be used in updateParams\n",
    "          : weight_decay: the weight decay to be used in updateParams - float\n",
    "    Outputs: W: the weights updated -  ndarray\n",
    "           : b: the bias updated -  ndarray\n",
    "    \"\"\"\n",
    "    grad_w = grad@X\n",
    "    grad_b = grad\n",
    "        \n",
    "    W = updateParams(W, grad_w, eta, regularizer, weight_decay)\n",
    "    b = updateParams(b, grad_b, eta, regularizer, weight_decay)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule is affected by regularization. We implement two cases: No regularization, or L2 regularization. Use the two possible update rules to implement the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(param, grad_param, eta, regularizer=None, weight_decay=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: param: the network parameters - ndarray\n",
    "          : grad_param: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : weight_decay: the weight-decay - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    if regularizer==None:\n",
    "        return param-eta*grad_param\n",
    "    elif regularizer=='L2':\n",
    "        return (1 - 2*weight_decay)*param - eta*grad_param\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Accuracy\n",
    "\n",
    "Here, we simply use the model to predict the class (by taking the argmax of the output !) for every example in ```X```, and count the number of times the model is right, to output the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAcc(W, b, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    ### Forward propagation\n",
    "    z = forward(W,b,X)\n",
    "    \n",
    "    ### Compute the softmax and the prediction\n",
    "    out = softmax(z)\n",
    "    pred = np.argmax(z,axis=0)\n",
    "    \n",
    "    ### Compute the accuracy\n",
    "    accuracy = len(pred[pred==labels])/len(pred)\n",
    "      \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training\n",
    "\n",
    "The following hyperparameters are given. Next, we can assemble all the function previously defined to implement a training loop. We will train the classifier on **one epoch**, meaning that the model will see each trainin example once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "eta = 0.01\n",
    "regularizer = 'L2'\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Training\n",
    "log_interval = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/9_dnczj57_v6pg3x4wd0s33w0000gn/T/ipykernel_18498/1464890385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m### Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m### Compute the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mz/9_dnczj57_v6pg3x4wd0s33w0000gn/T/ipykernel_18498/2387808166.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(W, b, X)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mOutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "# Data structures for plotting\n",
    "g_train_acc=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#######################\n",
    "### Learning process ##\n",
    "#######################\n",
    "n_training = len(train_images)\n",
    "for j in range(n_training):\n",
    "    ### Getting the example\n",
    "    X, y = train_images[j],y_train[j]\n",
    "\n",
    "    ### Forward propagation\n",
    "    W,b,X = initLayer(784,10)\n",
    "    z = forward(W,b,X)\n",
    "\n",
    "    ### Compute the softmax\n",
    "    out = softmax(z)\n",
    "        \n",
    "    ### Compute the gradient at the top layer\n",
    "    derror = out - y # This is o - y \n",
    "\n",
    "    ### Update the parameters\n",
    "    W, b = update(eta, W, b, grad, X, regularizer, weight_decay)\n",
    "\n",
    "    if j % log_interval == 0:\n",
    "        ### Every log_interval examples, look at the training accuracy\n",
    "        train_accuracy = computeAcc(W, b, train_images, train_labels) \n",
    "\n",
    "        ### And the testing accuracy\n",
    "        test_accuracy = computeAcc(W, b, test_images, test_labels) \n",
    "\n",
    "        g_train_acc.append(train_accuracy)\n",
    "        g_valid_acc.append(test_accuracy)\n",
    "        result_line = str(int(j)) + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "        print(result_line)\n",
    "\n",
    "g_train_acc.append(train_accuracy)\n",
    "g_valid_acc.append(valid_accuracy)\n",
    "result_line = \"Final result:\" + \" \" + str(train_accuracy) + \" \" + str(valid_accuracy) + \" \" + str(eta)\n",
    "print(result_line)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the performance of this simple linear classifier ? "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP4_1_empty.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
